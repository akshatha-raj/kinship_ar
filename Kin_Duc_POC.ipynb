{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+TXSgNcMg1htmqNExdkiR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akshatha-raj/kinship_ar/blob/main/Kin_Duc_POC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Cdej27xDETnf"
      },
      "outputs": [],
      "source": [
        "# prompt: use notebook magic command to save the output\n",
        "\n",
        "%%capture log_install_lib\n",
        "# install the required library\n",
        "!pip install openai\n",
        "!pip install cryptography\n",
        "!pip install torch\n",
        "!pip install psutil\n",
        "!pip install transformers\n",
        "!pip install diffusers\n",
        "!pip install accelerate\n",
        "# and other deploy lib\n",
        "!pip install gradio\n",
        "!pip install huggingface_hub\n",
        "# # for testing gpu speed\n",
        "# !pip install flopth\n",
        "# # other measuring lib\n",
        "# !pip install pynvml\n",
        "# !pip install py-cpuinfo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile app.py\n",
        "\n",
        "## required lib, required \"pip install\"\n",
        "# import transformers\n",
        "# import accelerate\n",
        "import openai\n",
        "import torch\n",
        "import cryptography\n",
        "import cryptography.fernet\n",
        "## interface libs, required \"pip install\"\n",
        "import gradio\n",
        "import huggingface_hub\n",
        "import huggingface_hub.hf_api\n",
        "## standard libs, no need to install\n",
        "import json\n",
        "import requests\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import sys\n",
        "import psutil\n",
        "import threading\n",
        "import socket\n",
        "# import PIL\n",
        "# import pandas\n",
        "import matplotlib\n",
        "class HFace_Pluto(object):\n",
        "  #\n",
        "  # initialize the object\n",
        "  def __init__(self, name=\"Pluto\",*args, **kwargs):\n",
        "    super(HFace_Pluto, self).__init__(*args, **kwargs)\n",
        "    self.author = \"Duc Haba\"\n",
        "    self.name = name\n",
        "    self._ph()\n",
        "    self._pp(\"Hello from class\", str(self.__class__) + \" Class: \" + str(self.__class__.__name__))\n",
        "    self._pp(\"Code name\", self.name)\n",
        "    self._pp(\"Author is\", self.author)\n",
        "    self._ph()\n",
        "    #\n",
        "    # define class var for stable division\n",
        "    self._device = 'cuda'\n",
        "    self._steps = [3,8,21,55,89,144]\n",
        "    self._guidances = [1.1,3.0,5.0,8.0,13.0,21.0]\n",
        "    self._xkeyfile = '.xoxo'\n",
        "    self._models = []\n",
        "    self._seed = 667 # sum of walnut in ascii (or Angle 667)\n",
        "    self._width = 512\n",
        "    self._height = 512\n",
        "    self._step = 50\n",
        "    self._guidances = 7.5\n",
        "    #self._generator = torch.Generator(device='cuda')\n",
        "    self.pipes = []\n",
        "    self.prompts = []\n",
        "    self.images = []\n",
        "    self.seeds = []\n",
        "    self.fname_id = 0\n",
        "    self.dname_img = \"img_colab/\"\n",
        "    self._huggingface_key=\"gAAAAABlSLcOFhWeLflLgoFsoP--xU03FbyL2LeA54IijE7UAR5AvO_Rd5V-vbXI71QcSj1VAnLwaLaX3DcTQ9y4pzAspUW7ftiwjJ066MhOnHfFx6unRO1DSMBshv2ltE8Vy_1qxQSD\"\n",
        "    self._gpt_key=\"gAAAAABlSLHjjdsu6AfFmwqwlaeHZTBNWM23KsznCbV7Krt-eyNrBqTjTgVY4JqNIGLHbtnJqFMWYDG8HkUQ7RQKR5SjRwpWFivKBRm3nlPr4sz93Ll2GYLCschxMoDUH71X3jEiVooijjLMyRLzl7gd0DTWX443OA==\"\n",
        "    self._fkey=\"fes_f8Im569hYnI1Tn6FqP-6hS4rdmNOJ6DWcRPOsvc=\"\n",
        "    self._color_primary = '#2780e3' #blue\n",
        "    self._color_secondary = '#373a3c' #dark gray\n",
        "    self._color_success = '#3fb618' #green\n",
        "    self._color_info = '#9954bb' #purple\n",
        "    self._color_warning = '#ff7518' #orange\n",
        "    self._color_danger = '#ff0039' #red\n",
        "    self._color_mid_gray = '#495057'\n",
        "    return\n",
        "  #\n",
        "  # pretty print output name-value line\n",
        "  def _pp(self, a, b,is_print=True):\n",
        "    # print(\"%34s : %s\" % (str(a), str(b)))\n",
        "    x = f'{\"%34s\" % str(a)} : {str(b)}'\n",
        "    y = None\n",
        "    if (is_print):\n",
        "      print(x)\n",
        "    else:\n",
        "      y = x\n",
        "    return y\n",
        "  #\n",
        "  # pretty print the header or footer lines\n",
        "  def _ph(self,is_print=True):\n",
        "    x = f'{\"-\"*34} : {\"-\"*34}'\n",
        "    y = None\n",
        "    if (is_print):\n",
        "      print(x)\n",
        "    else:\n",
        "      y = x\n",
        "    return y\n",
        "  #\n",
        "  # fetch huggingface file\n",
        "  def fetch_hface_files(self,\n",
        "    hf_names,\n",
        "    hf_space=\"duchaba/monty\",\n",
        "    local_dir=\"/content/\"):\n",
        "    f = str(hf_names) + \" is not iteratable, type: \" + str(type(hf_names))\n",
        "    try:\n",
        "      for f in hf_names:\n",
        "        lo = local_dir + f\n",
        "        huggingface_hub.hf_hub_download(repo_id=hf_space, filename=f,\n",
        "          use_auth_token=True,repo_type=huggingface_hub.REPO_TYPE_SPACE,\n",
        "          force_filename=lo)\n",
        "    except:\n",
        "      self._pp(\"*Error\", f)\n",
        "    return\n",
        "  #\n",
        "  #\n",
        "  def push_hface_files(self,\n",
        "    hf_names,\n",
        "    hf_space=\"duchaba/skin_cancer_diagnose\",\n",
        "    local_dir=\"/content/\"):\n",
        "    f = str(hf_names) + \" is not iteratable, type: \" + str(type(hf_names))\n",
        "    try:\n",
        "      for f in hf_names:\n",
        "        lo = local_dir + f\n",
        "        huggingface_hub.upload_file(\n",
        "          path_or_fileobj=lo,\n",
        "          path_in_repo=f,\n",
        "          repo_id=hf_space,\n",
        "          repo_type=huggingface_hub.REPO_TYPE_SPACE)\n",
        "    except Exception as e:\n",
        "      self._pp(\"*Error\", e)\n",
        "    return\n",
        "  #\n",
        "  # Define a function to display available CPU and RAM\n",
        "  def fetch_system_info(self):\n",
        "    s=''\n",
        "    # Get CPU usage as a percentage\n",
        "    cpu_usage = psutil.cpu_percent()\n",
        "    # Get available memory in bytes\n",
        "    mem = psutil.virtual_memory()\n",
        "    # Convert bytes to gigabytes\n",
        "    mem_total_gb = mem.total / (1024 ** 3)\n",
        "    mem_available_gb = mem.available / (1024 ** 3)\n",
        "    mem_used_gb = mem.used / (1024 ** 3)\n",
        "    # Print the results\n",
        "    s += f\"CPU usage: {cpu_usage}%\\n\"\n",
        "    s += f\"Total memory: {mem_total_gb:.2f} GB\\n\"\n",
        "    s += f\"Available memory: {mem_available_gb:.2f} GB\\n\"\n",
        "    # print(f\"Used memory: {mem_used_gb:.2f} GB\")\n",
        "    s += f\"Memory usage: {mem_used_gb/mem_total_gb:.2f}%\\n\"\n",
        "    return s\n",
        "  #\n",
        "  def restart_script_periodically(self):\n",
        "    while True:\n",
        "      #random_time = random.randint(540, 600)\n",
        "      random_time = random.randint(15800, 21600)\n",
        "      time.sleep(random_time)\n",
        "      os.execl(sys.executable, sys.executable, *sys.argv)\n",
        "    return\n",
        "  #\n",
        "  def write_file(self,fname, txt):\n",
        "    f = open(fname, \"w\")\n",
        "    f.writelines(\"\\n\".join(txt))\n",
        "    f.close()\n",
        "    return\n",
        "  #\n",
        "  def fetch_gpu_info(self):\n",
        "    s=''\n",
        "    try:\n",
        "      s += f'Your GPU is the {torch.cuda.get_device_name(0)}\\n'\n",
        "      s += f'GPU ready staus {torch.cuda.is_available()}\\n'\n",
        "      s += f'GPU allocated RAM: {round(torch.cuda.memory_allocated(0)/1024**3,1)} GB\\n'\n",
        "      s += f'GPU reserved RAM {round(torch.cuda.memory_reserved(0)/1024**3,1)} GB\\n'\n",
        "    except Exception as e:\n",
        "      s += f'**Warning, No GPU: {e}'\n",
        "    return s\n",
        "  #\n",
        "  def _fetch_crypt(self,is_generate=False):\n",
        "    s=self._fkey\n",
        "    if (is_generate):\n",
        "      s=open(self._xkeyfile, \"rb\").read()\n",
        "    return s\n",
        "  #\n",
        "  def _gen_key(self):\n",
        "    key = cryptography.fernet.Fernet.generate_key()\n",
        "    with open(self._xkeyfile, \"wb\") as key_file:\n",
        "        key_file.write(key)\n",
        "    return\n",
        "  #\n",
        "  def _decrypt_it(self, x):\n",
        "    y = self._fetch_crypt()\n",
        "    f = cryptography.fernet.Fernet(y)\n",
        "    m = f.decrypt(x)\n",
        "    return m.decode()\n",
        "  # from cryptography.fernet import Fernet\n",
        "  def _encrypt_it(self, x):\n",
        "    key = self._fetch_crypt()\n",
        "    p = x.encode()\n",
        "    f = cryptography.fernet.Fernet(key)\n",
        "    y = f.encrypt(p)\n",
        "    return y\n",
        "  #\n",
        "  def _login_hface(self):\n",
        "    try:\n",
        "      huggingface_hub.login(self._decrypt_it(self._huggingface_key),\n",
        "        add_to_git_credential=True) # non-blocking login\n",
        "      openai.api_key = self._decrypt_it(self._gpt_key)\n",
        "    except Exception as e:\n",
        "      print(f'Error: {e}')\n",
        "    self._ph()\n",
        "    return\n",
        "  #\n",
        "  def _fetch_version(self):\n",
        "    s = ''\n",
        "    print(f\"{'torch: 2.0.1':<25} Actual: {torch.__version__}\")\n",
        "    # print(f\"{'transformers: 4.29.2':<25} Actual: {transformers.__version__}\")\n",
        "    s += f\"{'openai: 0.27.7,':<28} Actual: {openai.__version__}\\n\"\n",
        "    s += f\"{'huggingface_hub: 0.14.1,':<28} Actual: {huggingface_hub.__version__}\\n\"\n",
        "    s += f\"{'gradio: 3.32.0,':<28} Actual: {gradio.__version__}\\n\"\n",
        "    s += f\"{'cryptography: 3.0.2,':<28} cryptography: {gradio.__version__}\\n\"\n",
        "\n",
        "    return s\n",
        "  #\n",
        "  def _fetch_host_ip(self):\n",
        "    s=''\n",
        "    hostname = socket.gethostname()\n",
        "    ip_address = socket.gethostbyname(hostname)\n",
        "    s += f\"Hostname: {hostname}\\n\"\n",
        "    s += f\"IP Address: {ip_address}\\n\"\n",
        "    return s\n",
        "  # parse the answer\n",
        "  def get_answer(self, resp, index=0):\n",
        "    return resp.get('choices')[index].get('text')\n",
        "  # print out the answer\n",
        "  def print_answer(self, resp, index=0,is_print_json=False):\n",
        "    print('----------')\n",
        "    print('The Answer')\n",
        "    print('----------')\n",
        "    rdata = self.get_answer(resp, index)\n",
        "    # print(textwrap.fill(rdata, width=72, replace_whitespace=False))\n",
        "    print(rdata)\n",
        "    if (is_print_json):\n",
        "      print('----------')\n",
        "      print('JSON Response')\n",
        "      print('----------')\n",
        "      print(resp)\n",
        "    return\n",
        "  #\n",
        "  # ask me function\n",
        "  def ask_me(self, prompt,\n",
        "    model=\"text-davinci-003\",\n",
        "    suffix=None,\n",
        "    max_tokens=128,     # length of output, max=2048\n",
        "    temperature=1.0,    # randomness: 0 to 2.0, higher (2.0) is a lot of random\n",
        "    top_p=1.0,          # accurate: 0 to 1.0\n",
        "    n=1,                # number of output\n",
        "    stream=False,       # partial progress return\n",
        "    logprobs=None,      # log properbility of token\n",
        "    echo=False,         # include the prompt in the response\n",
        "    stop=None,          # stop process on this character\n",
        "    presence_penalty=0, # likelyhood of new topic: -2.0 to 2.0\n",
        "    frequency_penalty=0,# llikelyhood of repeat: -2.0 to 2.0\n",
        "    best_of=1,          # best of choices from \"n\" above\n",
        "    logit_bias=None,    # do not use this word\n",
        "    user='None',        # user name for reporting back to OpenAI\n",
        "    is_print_json=False,\n",
        "    is_return_val=False\n",
        "    ):\n",
        "    try:\n",
        "      response = openai.Completion.create(\n",
        "        prompt=prompt,\n",
        "        model=model,\n",
        "        suffix=suffix,\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        n=n,\n",
        "        stream=stream,\n",
        "        logprobs=logprobs,\n",
        "        echo=echo,\n",
        "        stop=stop,\n",
        "        presence_penalty=presence_penalty,\n",
        "        frequency_penalty=frequency_penalty,\n",
        "        best_of=best_of,\n",
        "        #logit_bias=logit_bias,\n",
        "        user=user\n",
        "      )\n",
        "      return_val = None\n",
        "      if (is_return_val):\n",
        "        return_val = response\n",
        "      else:\n",
        "        self.print_answer(response,is_print_json=is_print_json)\n",
        "      return return_val\n",
        "    except Exception as e:\n",
        "      print(f'Error on model {model}. {e}')\n",
        "  #\n",
        "  def talk_to_me(self, prompt,\n",
        "    model='gpt-3.5-turbo',          # model defaut to gpt-3.5-turbo\n",
        "    role='user',                    # role can be either \"system\", \"user\", or \"assistant\"\n",
        "    #                               # -- below params are fewer then ask_me()\n",
        "    max_tokens=128,                 # length of output, max=2048\n",
        "    temperature=1.0,                # randomness: 0 to 2.0, higher (2.0) is a lot of random\n",
        "    top_p=1.0,                      # accurate: 0 to 1.0\n",
        "    n=1,                            # number of output\n",
        "    stream=False,                   # partial progress return\n",
        "    stop=None,                      # stop process on this character\n",
        "    presence_penalty=0,             # likelyhood of new topic: -2.0 to 2.0\n",
        "    frequency_penalty=0,            # llikelyhood of repeat: -2.0 to 2.0\n",
        "    logit_bias=None,                # do not use this word\n",
        "    user='None',                    # user name for reporting back to OpenAI\n",
        "    is_print_json=False,\n",
        "    is_return_val=False,\n",
        "    is_return_conversation=False\n",
        "    ):\n",
        "    try:\n",
        "      if (self.data_chat is None):\n",
        "        self.data_chat = [{'role': 'system','content':'It is a wonderful day.'},\n",
        "          {'role': role,'content':prompt}]\n",
        "      else:\n",
        "        self.data_chat.append({'role': role,'content':prompt})\n",
        "      #\n",
        "      response = openai.ChatCompletion.create(model=model,\n",
        "      messages=self.data_chat,\n",
        "      max_tokens=max_tokens,\n",
        "      temperature=temperature,\n",
        "      top_p=top_p,\n",
        "      n=n,\n",
        "      stream=stream,\n",
        "      stop=stop,\n",
        "      presence_penalty=presence_penalty,\n",
        "      frequency_penalty=frequency_penalty,\n",
        "      #logit_bias=logit_bias,\n",
        "      user=user\n",
        "      )\n",
        "      return_msg = response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "      err = f'Error: {e}'\n",
        "      response = err\n",
        "    #\n",
        "    try:\n",
        "      r = response.choices[0].message.role\n",
        "      self.data_chat.append({'role': r,'content':return_msg})\n",
        "    except Exception as e:\n",
        "      print(f'Error: {e}')\n",
        "    #\n",
        "    return_val = None\n",
        "    if (is_return_val):\n",
        "      return_val = response\n",
        "    elif (is_return_conversation):\n",
        "      return_val = self.data_chat\n",
        "    else:\n",
        "      print(return_msg)\n",
        "      if (is_print_json):\n",
        "        print(response)\n",
        "    return return_val\n",
        "    #\n",
        "# add module/method\n",
        "#\n",
        "import functools\n",
        "def add_method(cls):\n",
        "  def decorator(func):\n",
        "    @functools.wraps(func)\n",
        "    def wrapper(*args, **kwargs):\n",
        "      return func(*args, **kwargs)\n",
        "    setattr(cls, func.__name__, wrapper)\n",
        "    return func # returning func means func can still be used normally\n",
        "  return decorator\n",
        "#\n",
        "monty = HFace_Pluto(\"Monty\")\n",
        "monty._login_hface()\n",
        "print(monty._fetch_version())\n",
        "monty._ph()\n",
        "print(monty.fetch_system_info())\n",
        "monty._ph()\n",
        "print(monty.fetch_gpu_info())\n",
        "monty._ph()\n",
        "print(monty._fetch_host_ip())\n",
        "monty._ph()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJlK7SVWIjpq",
        "outputId": "9d3e5117-9433-4565-f82b-eacb05b2405d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------- : ----------------------------------\n",
            "                  Hello from class : <class '__main__.HFace_Pluto'> Class: HFace_Pluto\n",
            "                         Code name : Monty\n",
            "                         Author is : Duc Haba\n",
            "---------------------------------- : ----------------------------------\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n",
            "---------------------------------- : ----------------------------------\n",
            "torch: 2.0.1              Actual: 2.1.0+cu118\n",
            "openai: 0.27.7,              Actual: 0.28.1\n",
            "huggingface_hub: 0.14.1,     Actual: 0.17.3\n",
            "gradio: 3.32.0,              Actual: 4.1.1\n",
            "cryptography: 3.0.2,         cryptography: 4.1.1\n",
            "\n",
            "---------------------------------- : ----------------------------------\n",
            "CPU usage: 16.9%\n",
            "Total memory: 12.68 GB\n",
            "Available memory: 11.31 GB\n",
            "Memory usage: 0.08%\n",
            "\n",
            "---------------------------------- : ----------------------------------\n",
            "**Warning, No GPU: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx\n",
            "---------------------------------- : ----------------------------------\n",
            "Hostname: 8af0c25f8492\n",
            "IP Address: 172.28.0.12\n",
            "\n",
            "---------------------------------- : ----------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "monty._gpt_crkey = monty._encrypt_it('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')\n",
        "print(monty._gpt_crkey)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FX4dXVWJbPW",
        "outputId": "002c70d0-87a0-4e11-b23b-8da04b510a78"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'gAAAAABlSLHjjdsu6AfFmwqwlaeHZTBNWM23KsznCbV7Krt-eyNrBqTjTgVY4JqNIGLHbtnJqFMWYDG8HkUQ7RQKR5SjRwpWFivKBRm3nlPr4sz93Ll2GYLCschxMoDUH71X3jEiVooijjLMyRLzl7gd0DTWX443OA=='\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "monty._huggingface_key = monty._encrypt_it('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')\n",
        "print(monty._huggingface_key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iG6odL7fJniI",
        "outputId": "1d2e1f4e-2640-4672-ddc3-dd6e688574af"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'gAAAAABlSLcOFhWeLflLgoFsoP--xU03FbyL2LeA54IijE7UAR5AvO_Rd5V-vbXI71QcSj1VAnLwaLaX3DcTQ9y4pzAspUW7ftiwjJ066MhOnHfFx6unRO1DSMBshv2ltE8Vy_1qxQSD'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@add_method(HFace_Pluto)\n",
        "def list_openai_models(self):\n",
        "  models = openai.Model.list()\n",
        "  _mod = models.values()\n",
        "  import json\n",
        "  for y in _mod:\n",
        "    for z in y:\n",
        "      try:\n",
        "        print(z['id'])\n",
        "      except:\n",
        "        print('.')\n",
        "  return"
      ],
      "metadata": {
        "id": "cXv2vjBMJsaR"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test it\n",
        "monty.list_openai_models()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a9ig_D4Jwbr",
        "outputId": "3cdafcae-36a2-41b1-a37a-8c7fb83b0a3a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            "text-search-babbage-doc-001\n",
            "gpt-3.5-turbo-16k-0613\n",
            "curie-search-query\n",
            "gpt-3.5-turbo-16k\n",
            "text-search-babbage-query-001\n",
            "babbage\n",
            "babbage-search-query\n",
            "text-babbage-001\n",
            "whisper-1\n",
            "text-similarity-davinci-001\n",
            "davinci-similarity\n",
            "code-davinci-edit-001\n",
            "curie-similarity\n",
            "babbage-search-document\n",
            "curie-instruct-beta\n",
            "text-search-ada-doc-001\n",
            "davinci-instruct-beta\n",
            "text-similarity-babbage-001\n",
            "text-search-davinci-doc-001\n",
            "babbage-similarity\n",
            "text-embedding-ada-002\n",
            "davinci-search-query\n",
            "text-similarity-curie-001\n",
            "text-davinci-001\n",
            "text-search-davinci-query-001\n",
            "ada-search-document\n",
            "ada-code-search-code\n",
            "babbage-002\n",
            "davinci-002\n",
            "davinci-search-document\n",
            "curie-search-document\n",
            "babbage-code-search-code\n",
            "text-search-ada-query-001\n",
            "code-search-ada-text-001\n",
            "babbage-code-search-text\n",
            "code-search-babbage-code-001\n",
            "ada-search-query\n",
            "ada-code-search-text\n",
            "text-search-curie-query-001\n",
            "text-davinci-002\n",
            "text-davinci-edit-001\n",
            "code-search-babbage-text-001\n",
            "gpt-3.5-turbo\n",
            "gpt-3.5-turbo-instruct-0914\n",
            "ada\n",
            "text-ada-001\n",
            "ada-similarity\n",
            "code-search-ada-code-001\n",
            "text-similarity-ada-001\n",
            "gpt-3.5-turbo-0301\n",
            "gpt-3.5-turbo-instruct\n",
            "text-search-curie-doc-001\n",
            "text-davinci-003\n",
            "gpt-4-0613\n",
            "gpt-4\n",
            "text-curie-001\n",
            "curie\n",
            "gpt-4-0314\n",
            "davinci\n",
            "dall-e-2\n",
            "gpt-3.5-turbo-0613\n",
            "davinci:ft-code-and-theory-2023-08-03-02-26-13\n",
            "davinci:ft-code-and-theory-2023-08-02-20-31-21\n",
            "curie:ft-code-and-theory-2023-08-02-23-49-47\n",
            "curie:ft-code-and-theory-2023-08-04-02-22-20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: modify list_openai_models function to add data to pandas dataframe\n",
        "\n",
        "import pandas\n",
        "@add_method(HFace_Pluto)\n",
        "def list_openai_models(self):\n",
        "  models = openai.Model.list()\n",
        "  _mod = models.values()\n",
        "  df = pandas.DataFrame()\n",
        "  for y in _mod:\n",
        "    for z in y:\n",
        "      # print(z)\n",
        "      try:\n",
        "        df = pandas.concat([df, pandas.DataFrame({'id': z['id'], 'object': z['object'], 'created': z['created'],\n",
        "          'owned_by': z['owned_by'], 'permission': z['permission'],\n",
        "          'root': z['root'], 'parent': z['parent']})], ignore_index=True)\n",
        "      except:\n",
        "        print('.')\n",
        "  return df"
      ],
      "metadata": {
        "id": "yGVN42fkF608"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "monty.df_openai_models = monty.list_openai_models()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGtCvmqMF-4z",
        "outputId": "3edbeb92-cd23-4e1a-a1d7-b21220fda9dc"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "monty.df_openai_models.head(8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "CVO5pbulGC6r",
        "outputId": "8a60f4b5-5388-4deb-dc3b-0083282b091f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: []\n",
              "Index: []"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-19a9289c-8bd5-4ad4-b772-f15935b4913d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-19a9289c-8bd5-4ad4-b772-f15935b4913d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-19a9289c-8bd5-4ad4-b772-f15935b4913d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-19a9289c-8bd5-4ad4-b772-f15935b4913d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: sort pandas monty.df_openai_models by root column\n",
        "\n",
        "monty.df_openai_models.sort_values(by='root', ascending=False, inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "5p75vPaqIhio",
        "outputId": "1b25d8a5-915b-4a8d-9bd3-dcb658eecbcb"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-2c92dea2da8e>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# prompt: sort pandas monty.df_openai_models by root column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmonty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf_openai_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'root'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36msort_values\u001b[0;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[1;32m   6910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6911\u001b[0m             \u001b[0mby\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6912\u001b[0;31m             \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6914\u001b[0m             \u001b[0;31m# need to rewrap column in Series to apply key function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_label_or_level_values\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1848\u001b[0m             )\n\u001b[1;32m   1849\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1850\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'root'"
          ]
        }
      ]
    }
  ]
}